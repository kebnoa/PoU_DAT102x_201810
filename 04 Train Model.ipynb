{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 Train Model\n",
    "Used the cleaned, scaled, and normalised data to train the model.\n",
    "\n",
    "**Disclaimer.** I used AzureML for this step. My trail expired one week before the end of the competition so I started using SciKit Learn. I submitted results from here to the competition but never bettered the score I got via AzureML even though the RMSE scores were comparable. I can only assume that I was overtraining here!\n",
    "\n",
    "## Initialise the styles for the workbooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width: 90%;\n",
       "/*        margin-left:auto;*/\n",
       "/*        margin-right:auto;*/\n",
       "    }\n",
       "    ul {\n",
       "        line-height: 145%;\n",
       "        font-size: 90%;\n",
       "    }\n",
       "    li {\n",
       "        margin-bottom: 1em;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "    }\n",
       "    h4{\n",
       "        margin-top: 12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width: 90%;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise styles and packages we need\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and classes used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version:       0.23.4\n"
     ]
    }
   ],
   "source": [
    "# All the imports used\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(\"Pandas version:       {}\".format(pd.__version__))\n",
    "#print(\"Scikit learn version: {}\".format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import cleaned, scaled and normalised data we created in 03 Data Scaling and Normalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training values: (1311, 19)\n",
      "Training label: (1401, 2)\n",
      "Test values:     (616, 19)\n",
      "   row_id country_code  year  agricultural_land_area  forest_area  \\\n",
      "0       0      889f053  2002                0.644849     0.326591   \n",
      "1       1      9e614ab  2012                0.393423     0.598121   \n",
      "2       2      100c476  2000                0.013088     0.099657   \n",
      "3       3      4609682  2013                0.545174     0.371419   \n",
      "4       4      be2a7f5  2008                0.059169     0.177130   \n",
      "\n",
      "   total_land_area  population_growth  avg_value_of_food_production  \\\n",
      "0         0.522760           0.555582                      0.221955   \n",
      "1         0.435434           0.463276                      0.583676   \n",
      "2         0.017267           0.515693                      0.360597   \n",
      "3         0.396593           0.455697                      0.647380   \n",
      "4         0.039847           0.379837                      0.672717   \n",
      "\n",
      "   food_imports_as_share_of_merch_exports  \\\n",
      "0                                0.412590   \n",
      "1                                0.270031   \n",
      "2                                0.573186   \n",
      "3                                0.314796   \n",
      "4                                0.615790   \n",
      "\n",
      "   gross_domestic_product_per_capita_ppp  per_capita_food_supply_variability  \\\n",
      "0                               0.343225                            0.348034   \n",
      "1                               0.367934                            0.495084   \n",
      "2                               0.644420                            0.348249   \n",
      "3                               0.534370                            0.356623   \n",
      "4                               0.528860                            0.710241   \n",
      "\n",
      "   avg_supply_of_protein_of_animal_origin  \\\n",
      "0                                0.247253   \n",
      "1                                0.310009   \n",
      "2                                0.729224   \n",
      "3                                0.511734   \n",
      "4                                0.768029   \n",
      "\n",
      "   caloric_energy_from_cereals_roots_tubers  access_to_improved_sanitation  \\\n",
      "0                                  0.672182                       0.360459   \n",
      "1                                  0.739436                       0.581735   \n",
      "2                                  0.089336                       0.825776   \n",
      "3                                  0.453211                       0.896377   \n",
      "4                                  0.189920                       0.770917   \n",
      "\n",
      "   access_to_improved_water_sources  obesity_prevalence  \\\n",
      "0                          0.381366            0.339888   \n",
      "1                          0.566625            0.113026   \n",
      "2                          0.938416            0.595430   \n",
      "3                          0.910661            0.681353   \n",
      "4                          0.898450            0.621013   \n",
      "\n",
      "   access_to_electricity  co2_emissions  ratio_urban_population_total  \n",
      "0               0.514125       0.501945                      0.269132  \n",
      "1               0.762074       0.281082                      0.351608  \n",
      "2               0.893837       0.120386                      0.319509  \n",
      "3               0.971112       0.558871                      0.650169  \n",
      "4               0.901481       0.028361                      0.669631  \n"
     ]
    }
   ],
   "source": [
    "final_scaled_normalised_training_values_filename = 'data/DAT102x_Predicting_Chronic_Hunger_-_Clean_Normal_Training_values.csv'\n",
    "training_labels_filename = 'data/DAT102x_Predicting_Chronic_Hunger_-_Training_labels.csv'\n",
    "final_scaled_normalised_test_values_filename = 'data/DAT102x_Predicting_Chronic_Hunger_-_Clean_Normal_Test_values.csv'\n",
    "\n",
    "training_values = pd.read_csv(final_scaled_normalised_training_values_filename)\n",
    "training_labels = pd.read_csv(training_labels_filename)\n",
    "test_values = pd.read_csv(final_scaled_normalised_test_values_filename)\n",
    "\n",
    "# Makes sure country_code and year are treated as categorical!\n",
    "training_values['country_code'] = training_values['country_code'].astype('category')\n",
    "training_values['year'] = training_values['year'].astype('category')\n",
    "test_values['country_code'] = test_values['country_code'].astype('category')\n",
    "test_values['year'] = test_values['year'].astype('category')\n",
    "\n",
    "print(\"Training values: {}\".format(training_values.shape))\n",
    "print(\"Training label: {}\".format(training_labels.shape))\n",
    "print(\"Test values:     {}\".format(test_values.shape))\n",
    "print(training_values.head())\n",
    "#print(training_values.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join training features and label into test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1311, 20)\n"
     ]
    }
   ],
   "source": [
    "tempDF = pd.merge(training_values, training_labels, on='row_id', how='inner')\n",
    "print(tempDF.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the test feature matrix and test label vector.\n",
    "Exclude country_code from model and use get_dummies to convert year column into \"one hot encoding\" format. That is, we are treating year as a categorical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1311, 32)\n",
      "[31.26071279 18.29823274 39.51339713 ... 12.08848436 26.43666106\n",
      " 13.71256945]\n"
     ]
    }
   ],
   "source": [
    "# Start at 2nd column, i.e. exclude country_code\n",
    "X = pd.get_dummies(training_values.iloc[:,2:len(training_values)])\n",
    "y = tempDF['prevalence_of_undernourishment'].values\n",
    "print(X.shape)\n",
    "#print(X.dtypes)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use train/test split with different random_state values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create regressor using LBFGS (small training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPRegressor(activation='identity', alpha=0.0001, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=300, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=None, shuffle=True, solver='lbfgs', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "nn = MLPRegressor(activation='identity',\n",
    "                  hidden_layer_sizes=300,\n",
    "                  max_iter=500,\n",
    "                  verbose=False,\n",
    "                  solver='lbfgs')\n",
    "print(nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model and then use it to predict PoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.795935811861021\n"
     ]
    }
   ],
   "source": [
    "nn.fit(X_train, y_train)\n",
    "y_pred = nn.predict(X_test)\n",
    "print((metrics.mean_squared_error(y_test,y_pred))**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model sensitivy by running a 10 fold cross validation to get a sense of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 6.82 (+/- 5.84)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(nn, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "print(\"RMSE: %0.2f (+/- %0.2f)\" % ((abs(scores.mean())**0.5), (abs(scores.std())**0.5) * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPRegressor(activation='identity', alpha=0.0001, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=300, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=None, shuffle=True, solver='lbfgs', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to use a grid search to find \"best\" parameters\n",
    "# define the parameter values that should be searched\n",
    "hidden_layer_sizes_range = [300, 400, 500] # had 100, 200, 400, 500\n",
    "solver_options = [ 'lbfgs'] #, 'lbfgs', 'sgd',]\n",
    "activation_options = ['identity']#, 'logistic', 'tanh', 'relu']\n",
    "alpha_range = [0.0001]\n",
    "max_iter_range = [400, 500] # 200, 300, 500..\n",
    "#beta_1_range = [0.3, 0.5, 0.7, 0.9, 0.99]\n",
    "#beta_2_range = [0.5, 0.999]\n",
    "\n",
    "# create a parameter grid: map the parameter names to the values that should be searched\n",
    "param_grid = dict(hidden_layer_sizes=hidden_layer_sizes_range,\n",
    "                  solver=solver_options,\n",
    "                  alpha=alpha_range,\n",
    "                  activation=activation_options,\n",
    "                  max_iter=max_iter_range)\n",
    "#                  beta_1=beta_1_range,\n",
    "#                  beta_2=beta_2_range)\n",
    "#                  learning_rate_init=learning_rate_init_range,\n",
    "\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# instantiate and fit the grid\n",
    "grid = GridSearchCV(nn, param_grid, cv=10, \n",
    "                    scoring='neg_mean_squared_error', \n",
    "                    return_train_score=False,\n",
    "                    n_jobs = -1)\n",
    "grid.fit(X, y)\n",
    "\n",
    "#grid = GridSearchCV(nn, param_grid, cv=10, \n",
    "#                    scoring='neg_mean_squared_error', \n",
    "#                    return_train_score=False,\n",
    "#                    n_jobs = -1)\n",
    "\n",
    "# Best using lbfgs:\n",
    "#{'activation': 'identity', 'alpha': 0.0001, 'hidden_layer_sizes': 300, 'max_iter': 400, 'solver': 'lbfgs'}\n",
    "# Best using adam:\n",
    "#{'activation': 'identity', 'alpha': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'hidden_layer_sizes': 400, 'max_iter': 400, 'solver': 'adam'}\n",
    "\n",
    "# Choose to use lbfgs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(pd.DataFrame(grid.cv_results_)[['mean_test_score', 'std_test_score', 'params']])\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "access_to_improved_water_sources\n",
    "access_to_electricity\n",
    "obesity_prevalence\n",
    "access_to_improved_sanitation\n",
    "avg_supply_of_protein_of_animal_origin\n",
    "life_expectancy\n",
    "adult_literacy_rate\n",
    "avg_value_of_food_production\n",
    "\n",
    "droughts_floods_extreme_temps\n",
    "population_growth\n",
    "anemia_prevalence\n",
    "caloric_energy_from_cereals_roots_tubers\n",
    "net_oda_received_percent_gni\n",
    "open_defecation\n",
    "fertility_rate\n",
    "country_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All the testing below implies this is the BEST model, so use it to predict and check result?\n",
    "nn = MLPRegressor(activation='identity',\n",
    "                  hidden_layer_sizes=300,\n",
    "                  max_iter=500,\n",
    "                  verbose=False,\n",
    "                  solver='lbfgs')\n",
    "print(nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember to train the model on the whole dataset before applying it to the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.64778508637993\n"
     ]
    }
   ],
   "source": [
    "# Use the whole dataset to train the model\n",
    "nn.fit(X, y)\n",
    "y_pred = nn.predict(X)\n",
    "print((metrics.mean_squared_error(y,y_pred))**0.5)\n",
    "#print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the test dataset and predict the PoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(616, 32)\n",
      "[ 1.41724135  1.71842828  9.03670388 28.95772989 16.4509675  30.58808409\n",
      " 20.31616989 31.5342125  29.1354775  43.8337594   9.75434687 10.61178819\n",
      "  8.19449654 13.96173229 29.40501026 24.9647514  40.95813128 17.09449421\n",
      "  1.05596722 16.27264338 20.80315299 27.20354433 -1.08484217  3.29896022\n",
      "  7.32214516 25.15823944 13.75186781  6.66927417  6.40886237 14.84454743\n",
      "  9.70552576  8.80166268 13.06296089  7.34308647  7.42598916 32.12140703\n",
      " 39.04290912 10.77909172  0.48208604 35.49616987 16.93740981 32.69243354\n",
      " 13.71149386 13.45234485 -0.15768273 18.91311993 28.48756593  9.35881364\n",
      " 31.36378527 28.57655803  9.90920474 10.27753566 12.18869344 10.02371283\n",
      " 26.28649634 13.33645453  5.24064331  4.04948549 29.48631044  9.02753827\n",
      " 18.58593375 25.94044947 19.30992625 27.56233515 17.90805372  9.51863189\n",
      "  3.70757543  9.36426111  3.83234662 20.08227152 14.68252745 25.46711987\n",
      "  1.93853109  5.5693839  27.64648564 19.54327996 27.08022874 12.13709628\n",
      "  9.81476777  3.34248313 20.87988097 31.55095024 19.30745238 32.53335117\n",
      " 34.65118157 30.18078357 11.37742971 -0.3814854  -2.35306855 37.86879218\n",
      " -3.4759146  13.52068676 32.43648051 10.69379123 15.28533109  9.28072486\n",
      " 26.14163365 30.451499    1.64775076 13.01020595 14.00488257 10.73524173\n",
      " 33.01652449  7.26994031 26.83943936 32.20792022 -2.73787445  9.33459163\n",
      " 25.81186116 28.22551573 27.37645767 10.63280169 19.60840172 11.37408705\n",
      " 34.17375465 29.67001919  3.23347265 -2.42094744 13.46489914  6.23756352\n",
      "  9.6535877  36.72772563 13.70820517 32.56855943 10.70373899  6.45826167\n",
      " -2.88473256 -6.1796591  30.06328822  8.66049559 30.71572006 21.93765578\n",
      " 10.16525266 -4.72752764 22.28181971 45.84587187 33.21593472 19.03763711\n",
      "  8.53177203 29.71406024 -0.35114651 11.83873016 21.60942395 37.73365444\n",
      " 24.08294815 14.20358016  4.12945635  4.1355886  15.03931535 -6.60866797\n",
      " 27.29736176 12.06009576  4.1123008  15.48340241 13.55523517 22.98140144\n",
      " 30.17944468 33.62770813 11.51586637  7.69107589  3.63781114 36.76712645\n",
      " 19.57391886 -5.20469303 17.15798415 12.42200604  4.10751073 23.98485527\n",
      "  5.16807741 23.34867802 44.61110016  2.92849408 24.1287084   8.4243008\n",
      " 18.20592056 25.25093011  9.49249978  9.1188917  33.55098916  9.33841527\n",
      "  7.51150736 18.94907199 21.21613721 16.76400655 17.41855501 32.97837181\n",
      "  8.25933733  3.19961274  3.81026399 13.00348896 29.34811741  3.63049287\n",
      "  3.55526246 20.40254335 22.07656069  7.13118616 23.41515118 31.21786074\n",
      "  9.4764224  16.1157414  12.08537828 14.25949044 26.7909597  29.77991728\n",
      " 30.83466992  2.57815001 -5.04254516 13.1594917  34.19214433  9.78563435\n",
      " 34.26345194 16.1595649  12.29317402 14.02510641 31.43873864  3.82191714\n",
      " 45.76580705 22.26074104 16.75605144 21.82417504 31.95874162 36.67126673\n",
      "  0.42998309 33.15139069 17.67405406 25.98430642  7.00880087  4.7432117\n",
      " -3.78222187 25.98049785 34.4497343   1.21993941 24.36614606  9.87370063\n",
      "  1.37966735 35.5791836  15.06146263  0.64458356  8.62214022 26.481531\n",
      " 16.10301682  3.56323226 29.18353727 17.37761978 26.94837591 10.46978386\n",
      "  1.35886176  0.42709958 23.27700496  5.38622218  7.31421377 -0.67807323\n",
      "  7.00445315 34.82485556 13.29202612  3.13858178 10.85901723 32.6026087\n",
      " 28.15540735 13.28092308  8.16873097 26.14788681 13.49849096  3.69740661\n",
      "  3.10437824  0.79510702  8.6914315  25.43890341 15.19243565 33.54274552\n",
      " 17.79164883 25.86165065  9.08294578 -2.67162867 26.07632298 17.08234283\n",
      " 21.43030931 41.93054342 19.2670981  10.2808113  29.77641958 12.07632447\n",
      " 21.81145183 10.54972963 10.36851293 31.39785587 19.232337   28.2589809\n",
      " 19.45396973 25.68671139  9.98977694 11.57801945 18.15554674 21.11294936\n",
      " 29.11518831 20.21610308  6.64622532  9.93351158 11.33876401 22.75485822\n",
      " 12.94469504 33.50590691 23.98239933  6.79481259 19.07539255  1.87424631\n",
      " 37.3462955  12.26193357 32.05061548  9.91943083 20.03375954 25.49942097\n",
      " -5.64644596 22.07856617  0.66388086 33.52457572  4.3045472  24.24546212\n",
      " 35.76046943 13.33038172 -6.39309052 26.19675577 29.89992217 29.64004182\n",
      " 32.79174843 25.40560332  1.21966784 19.68975771 21.78475655  9.3657722\n",
      "  1.65944375 30.62441405 38.71504738  9.87513331 10.0570963  10.42656948\n",
      "  9.6586699  12.79123418 24.26567129 24.49609776 23.66168989  1.40109096\n",
      " 32.69553521 14.5470098  38.25916725 25.24869222 20.21837482  2.97884062\n",
      " 27.1836773  36.32037177 10.56004924  4.7524627  30.16857618 44.21479988\n",
      "  5.96465489  9.93505296  2.02566304 32.34478796 20.75980958 -0.22472914\n",
      " -0.09141975  5.32413156 33.40098196 16.49749772 15.34181738  1.93551617\n",
      " 12.13641794 19.16737081  8.87057224  7.5597302  41.96175487 16.62635181\n",
      "  5.851202   35.17353444 24.97100245 13.077218   27.01893135 30.94445478\n",
      "  4.38840918 30.66451992  5.9832145  11.91586814 37.00132813  1.5795979\n",
      " 13.66112202 30.23721041 13.78996538 14.4082169   3.60108571  7.6019607\n",
      "  9.87365135 24.60680926 22.65913038 21.1075051   1.77680034 -6.02203934\n",
      "  3.82905888 29.5205159   3.74090982  9.21872394 23.68401353 21.67539261\n",
      " 34.21954907 19.03679994 14.95283918 42.3919462  -3.39520251 19.61236794\n",
      "  7.23545801 25.35231014 24.64256774 24.62520536 11.90494553  3.2806339\n",
      "  7.9032841  44.0442772   9.86169441 25.43390966 25.75525782 17.77205848\n",
      " -1.856084   13.10075727 21.5182296  24.17706771  7.23921415 33.17071204\n",
      " -3.87914756 30.33324074 41.2175507  25.18062503 38.42491022 -5.33241942\n",
      " 10.47885251 36.28053961 -3.28542405 13.2779143  -0.74443639 12.66108829\n",
      " 29.2089719  -0.98839148 28.14571995  4.05204388 26.31488783 10.84458569\n",
      " 11.70029552 29.84665818  4.36705774 -0.35548406 -1.75071581 32.34327769\n",
      " 14.4585117  31.09267354 28.79299798 -3.9569802   4.45214542  9.76158637\n",
      " 23.51864896 14.28944562 22.76018273 28.01034904  3.49178864 21.67244531\n",
      " 25.59798248 10.19239792 25.21802717  3.07110567 29.99628402  3.52298593\n",
      "  7.77204251  4.85476842  8.62616836 -3.35965886 32.28909152 15.88007017\n",
      " 17.77949633 30.44610685 11.59788242 24.64541645 -6.02533522  0.90385959\n",
      "  3.90279119 27.57873013 30.14553055 -5.61737353 14.92226871 34.42474904\n",
      " 17.00997756 31.83220791 24.91169004 25.07221697 -5.34373035 19.75041161\n",
      " 36.13817961 30.46546789 20.38048919 14.67056906  5.50248852 17.42244021\n",
      " 14.00513219  6.03481617  2.07655628 29.35982889 27.10796499 16.47255869\n",
      "  1.88853774  2.11636992  9.27102572  3.54837449 10.30155264  4.40955068\n",
      " 10.48113354 -1.82697165  2.23780442 13.20233926 -6.18168063 14.39331618\n",
      " 24.34117283 27.50684114  7.78795462 35.77825303 42.73005689  3.20835108\n",
      "  9.44857944 42.599134   13.58592062 10.17341408 35.16264569  8.45544282\n",
      "  3.39106169 39.59863168  8.56839678 13.22323851  9.16059615 11.58733708\n",
      " 25.2614048  25.86388505 -2.78859073 18.54575326 21.13931384  5.42484339\n",
      " 43.26587857  5.56102513 16.2220748  10.9893173  11.08804842  7.26565734\n",
      "  1.15077187 46.81815788 10.67244194  4.96840481 10.71352341 -5.54453583\n",
      " 22.72677648 42.20457181 38.61299818 34.69585334  6.15103321  9.2606306\n",
      " 22.68995002 14.50580032 26.76219602  2.63426453  3.55223006 12.90690506\n",
      " 43.21749355  6.54929723 12.5523558  35.00313681 41.51038274 26.82936606\n",
      " 26.37853901  4.27406955 31.0565451  35.19080124 12.83297907 15.23819229\n",
      " 23.45825165 -0.98522485  4.92322501  6.24715538 11.22041586 17.76399505\n",
      " -0.62972717 18.32717719 11.92840279 25.48750995 18.88801578 35.81310177\n",
      " 34.57677824 26.61337392 -7.53180291 15.12642309  4.1940516   3.39891427\n",
      "  7.02096901 34.0455911  27.25307288 14.26116124 36.65522324 15.21638849\n",
      "  8.3708018  10.86799748  9.13513885  4.27874511 26.42829503 13.47602428\n",
      " 33.03681893  2.01192179 28.04953028 -1.62111959  5.22223639 30.44672577\n",
      "  1.22593866 23.67519216 -0.29620492 40.91600712]\n"
     ]
    }
   ],
   "source": [
    "X_test = pd.get_dummies(test_values.iloc[:,2:len(test_values)])\n",
    "print(X_test.shape)\n",
    "y_pred = nn.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the final output format and sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "   row_id  prevalence_of_undernourishment\n",
      "0       0                        1.417241\n",
      "1       1                        1.718428\n",
      "2       2                        9.036704\n",
      "3       3                       28.957730\n",
      "4       4                       16.450968\n"
     ]
    }
   ],
   "source": [
    "pred = pd.DataFrame(pd.Series(y_pred), columns=['prevalence_of_undernourishment'])\n",
    "row = pd.DataFrame(test_values['row_id'].copy(), columns=['row_id'])\n",
    "res = pd.concat([row, pred], axis='columns')\n",
    "print(type(pred))\n",
    "print(type(row))\n",
    "print(type(res))\n",
    "print(res.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to disk for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('data/prediction-20181030-01.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "This first attempt is good and scored an RMSE of 9.074 (3rd submission of 30th Oct 2018). The next step is to try and tune the model parameters to check if I can equal the AzureML one.\n",
    "\n",
    "If you look through the predicted values you can also see some that predict a negative PoU. The PoU can't be negative and some thought should be put into \"fixing\" these. Simplest idea is to simply make the zero."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
