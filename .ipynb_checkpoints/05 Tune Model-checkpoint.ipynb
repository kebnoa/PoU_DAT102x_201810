{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 Tune Model\n",
    "Used the cleaned, scaled, and normalised data to train the model.\n",
    "\n",
    "**Disclaimer.** I used AzureML for this step. My trail expired one week before the end of the competition so I started using SciKit Learn. I submitted results from here to the competition but never bettered the score I got via AzureML even though the RMSE scores were comparable. I can only assume that I was overtraining here!\n",
    "\n",
    "## Initialise the styles for the workbooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width: 90%;\n",
       "/*        margin-left:auto;*/\n",
       "/*        margin-right:auto;*/\n",
       "    }\n",
       "    ul {\n",
       "        line-height: 145%;\n",
       "        font-size: 90%;\n",
       "    }\n",
       "    li {\n",
       "        margin-bottom: 1em;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "    }\n",
       "    h4{\n",
       "        margin-top: 12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width: 90%;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise styles and packages we need\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and classes used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version:       0.23.4\n"
     ]
    }
   ],
   "source": [
    "# All the imports used\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(\"Pandas version:       {}\".format(pd.__version__))\n",
    "#print(\"Scikit learn version: {}\".format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import cleaned, scaled and normalised data we created in 03 Data Scaling and Normalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training values: (1311, 19)\n",
      "Training label: (1401, 2)\n",
      "Test values:     (616, 19)\n",
      "   row_id country_code  year  agricultural_land_area  forest_area  \\\n",
      "0       0      889f053  2002                0.644849     0.326591   \n",
      "1       1      9e614ab  2012                0.393423     0.598121   \n",
      "2       2      100c476  2000                0.013088     0.099657   \n",
      "3       3      4609682  2013                0.545174     0.371419   \n",
      "4       4      be2a7f5  2008                0.059169     0.177130   \n",
      "\n",
      "   total_land_area  population_growth  avg_value_of_food_production  \\\n",
      "0         0.522760           0.555582                      0.221955   \n",
      "1         0.435434           0.463276                      0.583676   \n",
      "2         0.017267           0.515693                      0.360597   \n",
      "3         0.396593           0.455697                      0.647380   \n",
      "4         0.039847           0.379837                      0.672717   \n",
      "\n",
      "   food_imports_as_share_of_merch_exports  \\\n",
      "0                                0.412590   \n",
      "1                                0.270031   \n",
      "2                                0.573186   \n",
      "3                                0.314796   \n",
      "4                                0.615790   \n",
      "\n",
      "   gross_domestic_product_per_capita_ppp  per_capita_food_supply_variability  \\\n",
      "0                               0.343225                            0.348034   \n",
      "1                               0.367934                            0.495084   \n",
      "2                               0.644420                            0.348249   \n",
      "3                               0.534370                            0.356623   \n",
      "4                               0.528860                            0.710241   \n",
      "\n",
      "   avg_supply_of_protein_of_animal_origin  \\\n",
      "0                                0.247253   \n",
      "1                                0.310009   \n",
      "2                                0.729224   \n",
      "3                                0.511734   \n",
      "4                                0.768029   \n",
      "\n",
      "   caloric_energy_from_cereals_roots_tubers  access_to_improved_sanitation  \\\n",
      "0                                  0.672182                       0.360459   \n",
      "1                                  0.739436                       0.581735   \n",
      "2                                  0.089336                       0.825776   \n",
      "3                                  0.453211                       0.896377   \n",
      "4                                  0.189920                       0.770917   \n",
      "\n",
      "   access_to_improved_water_sources  obesity_prevalence  \\\n",
      "0                          0.381366            0.339888   \n",
      "1                          0.566625            0.113026   \n",
      "2                          0.938416            0.595430   \n",
      "3                          0.910661            0.681353   \n",
      "4                          0.898450            0.621013   \n",
      "\n",
      "   access_to_electricity  co2_emissions  ratio_urban_population_total  \n",
      "0               0.514125       0.501945                      0.269132  \n",
      "1               0.762074       0.281082                      0.351608  \n",
      "2               0.893837       0.120386                      0.319509  \n",
      "3               0.971112       0.558871                      0.650169  \n",
      "4               0.901481       0.028361                      0.669631  \n"
     ]
    }
   ],
   "source": [
    "final_scaled_normalised_training_values_filename = 'data/DAT102x_Predicting_Chronic_Hunger_-_Clean_Normal_Training_values.csv'\n",
    "training_labels_filename = 'data/DAT102x_Predicting_Chronic_Hunger_-_Training_labels.csv'\n",
    "final_scaled_normalised_test_values_filename = 'data/DAT102x_Predicting_Chronic_Hunger_-_Clean_Normal_Test_values.csv'\n",
    "\n",
    "training_values = pd.read_csv(final_scaled_normalised_training_values_filename)\n",
    "training_labels = pd.read_csv(training_labels_filename)\n",
    "test_values = pd.read_csv(final_scaled_normalised_test_values_filename)\n",
    "\n",
    "# Makes sure country_code and year are treated as categorical!\n",
    "training_values['country_code'] = training_values['country_code'].astype('category')\n",
    "training_values['year'] = training_values['year'].astype('category')\n",
    "test_values['country_code'] = test_values['country_code'].astype('category')\n",
    "test_values['year'] = test_values['year'].astype('category')\n",
    "\n",
    "print(\"Training values: {}\".format(training_values.shape))\n",
    "print(\"Training label: {}\".format(training_labels.shape))\n",
    "print(\"Test values:     {}\".format(test_values.shape))\n",
    "print(training_values.head())\n",
    "#print(training_values.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join training features and label into test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1311, 20)\n"
     ]
    }
   ],
   "source": [
    "tempDF = pd.merge(training_values, training_labels, on='row_id', how='inner')\n",
    "print(tempDF.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the test feature matrix and test label vector.\n",
    "Exclude country_code from model and use get_dummies to convert year column into \"one hot encoding\" format. That is, we are treating year as a categorical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1311, 32)\n",
      "[31.26071279 18.29823274 39.51339713 ... 12.08848436 26.43666106\n",
      " 13.71256945]\n"
     ]
    }
   ],
   "source": [
    "# Start at 2nd column, i.e. exclude country_code\n",
    "X = pd.get_dummies(training_values.iloc[:,2:len(training_values)])\n",
    "y = tempDF['prevalence_of_undernourishment'].values\n",
    "print(X.shape)\n",
    "#print(X.dtypes)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use grid search to tune the model parameters\n",
    "From 04 Train Model we have a model with an score of \"RMSE: 6.82 (+/- 5.84)\", lets tune the neural network parameters to see if we can improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select parameters to use in grid search - Assuming 'adam' solver\n",
    "This is exponential, i.e. the cell below will execute the model training and prediction 10 * 3 * 1 * 2 * 5 * 2 = 600 times assuming a 10-fold cross validation. This will take a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': [300, 400, 500], 'alpha': [0.0001], 'max_iter': [400, 500], 'beta_1': [0.3, 0.5, 0.7, 0.9, 0.99], 'beta_2': [0.5, 0.999]}\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_sizes_range = [300, 400, 500]\n",
    "alpha_range = [0.0001]\n",
    "max_iter_range = [400, 500]\n",
    "beta_1_range = [0.3, 0.5, 0.7, 0.9, 0.99]\n",
    "beta_2_range = [0.5, 0.999]\n",
    "\n",
    "param_grid = dict(hidden_layer_sizes=hidden_layer_sizes_range,\n",
    "                  alpha=alpha_range, \n",
    "                  max_iter=max_iter_range,\n",
    "                  beta_1=beta_1_range,\n",
    "                  beta_2=beta_2_range)\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instanciate the grid and commence search.\n",
    "I used n_jobs = -1 to use all my CPUs to reduce waiting time. This really did hammer my CPUs so your mileage may vary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPRegressor(activation='identity',\n",
    "                  verbose=False,\n",
    "                  solver='adam')\n",
    "grid = GridSearchCV(nn,\n",
    "                    param_grid,\n",
    "                    cv=10,\n",
    "                    scoring='neg_mean_squared_error',\n",
    "                    return_train_score=False,\n",
    "                    n_jobs = -1)\n",
    "# uncomment below to repeat the exercise.\n",
    "#grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you wish to repeat\n",
    "\n",
    "#print(pd.DataFrame(grid.cv_results_)[['mean_test_score', 'std_test_score', 'params']])\n",
    "#print(grid.best_score_)\n",
    "## gave -47.05303823697013 gives RMSE of 6.8595\n",
    "#print(grid.best_params_)\n",
    "## gave {'alpha': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'hidden_layer_sizes': 500, 'max_iter': 400}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting results\n",
    "With the best \"adam\" solver the RMSE is 6.8595 which is worse than the 'lbfgs' solver we used in 04 Train Model.\n",
    "\n",
    "Repeat the above exercise for the 'lbfgs' solver and see if we can improve more.\n",
    "\n",
    "### Select parameters to use in grid search - Assuming 'lbfgs' solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': [300, 400, 500, 600], 'alpha': [1e-05, 0.0001, 0.001], 'max_iter': [300, 400, 500]}\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_sizes_range = [300, 400, 500, 600]\n",
    "alpha_range = [0.00001, 0.0001, 0.001]\n",
    "max_iter_range = [300, 400, 500]\n",
    "param_grid = dict(hidden_layer_sizes=hidden_layer_sizes_range,\n",
    "                  alpha=alpha_range, \n",
    "                  max_iter=max_iter_range)\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instanciate the grid and commence search.\n",
    "I used n_jobs = -1 to use all my CPUs to reduce waiting time. This really did hammer my CPUs so your mileage may vary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPRegressor(activation='identity',\n",
    "                  verbose=False,\n",
    "                  solver='lbfgs')\n",
    "grid = GridSearchCV(nn,\n",
    "                    param_grid,\n",
    "                    cv=10,\n",
    "                    scoring='neg_mean_squared_error',\n",
    "                    return_train_score=False,\n",
    "                    n_jobs = -1)\n",
    "# uncomment below to repeat the exercise.\n",
    "#grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you wish to repeat\n",
    "\n",
    "#print(pd.DataFrame(grid.cv_results_)[['mean_test_score', 'std_test_score', 'params']])\n",
    "#print(grid.best_score_)\n",
    "## gave -46.53876933228354 gives RMSE of 6.8219\n",
    "#print(grid.best_params_)\n",
    "## {'alpha': 0.001, 'hidden_layer_sizes': 500, 'max_iter': 500}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting the results\n",
    "With the best \"lbfgs\" solver the RMSE is 6.8219 which is the same as the the 'lbfgs' solver at 6.82 we used in 04 Train Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "This workbook contains the process whereby you can tune the model parameters. It is clear from this that the lbfgs solver is the better option and as we used in it \"04 Train Model\" the next steps are to look at better feature selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
