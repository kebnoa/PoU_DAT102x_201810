{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 Train Model\n",
    "Used the cleaned, scaled, and normalised data to train the model.\n",
    "\n",
    "**Disclaimer.** I used AzureML for this step. My trail expired one week before the end of the competition so I started using SciKit Learn. I submitted results from here to the competition but never bettered the score I got via AzureML even though the RMSE scores were comparable. I can only assume that I was overtraining here!\n",
    "\n",
    "## Initialise the styles for the workbooksÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width: 90%;\n",
       "/*        margin-left:auto;*/\n",
       "/*        margin-right:auto;*/\n",
       "    }\n",
       "    ul {\n",
       "        line-height: 145%;\n",
       "        font-size: 90%;\n",
       "    }\n",
       "    li {\n",
       "        margin-bottom: 1em;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "    }\n",
       "    h4{\n",
       "        margin-top: 12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width: 90%;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise styles and packages we need\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and classes used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version:       0.23.4\n"
     ]
    }
   ],
   "source": [
    "# All the imports used\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(\"Pandas version:       {}\".format(pd.__version__))\n",
    "#print(\"Scikit learn version: {}\".format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import cleaned, scaled and normalised data we created in 03 Data Scaling and Normalising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training values: (1311, 19)\n",
      "Training label: (1401, 2)\n",
      "Test values:     (616, 19)\n",
      "   row_id country_code  year  agricultural_land_area  forest_area  \\\n",
      "0       0      889f053  2002                0.644849     0.326591   \n",
      "1       1      9e614ab  2012                0.393423     0.598121   \n",
      "2       2      100c476  2000                0.013088     0.099657   \n",
      "3       3      4609682  2013                0.545174     0.371419   \n",
      "4       4      be2a7f5  2008                0.059169     0.177130   \n",
      "\n",
      "   total_land_area  population_growth  avg_value_of_food_production  \\\n",
      "0         0.522760           0.555582                      0.221955   \n",
      "1         0.435434           0.463276                      0.583676   \n",
      "2         0.017267           0.515693                      0.360597   \n",
      "3         0.396593           0.455697                      0.647380   \n",
      "4         0.039847           0.379837                      0.672717   \n",
      "\n",
      "   food_imports_as_share_of_merch_exports  \\\n",
      "0                                0.412590   \n",
      "1                                0.270031   \n",
      "2                                0.573186   \n",
      "3                                0.314796   \n",
      "4                                0.615790   \n",
      "\n",
      "   gross_domestic_product_per_capita_ppp  per_capita_food_supply_variability  \\\n",
      "0                               0.343225                            0.348034   \n",
      "1                               0.367934                            0.495084   \n",
      "2                               0.644420                            0.348249   \n",
      "3                               0.534370                            0.356623   \n",
      "4                               0.528860                            0.710241   \n",
      "\n",
      "   avg_supply_of_protein_of_animal_origin  \\\n",
      "0                                0.247253   \n",
      "1                                0.310009   \n",
      "2                                0.729224   \n",
      "3                                0.511734   \n",
      "4                                0.768029   \n",
      "\n",
      "   caloric_energy_from_cereals_roots_tubers  access_to_improved_sanitation  \\\n",
      "0                                  0.672182                       0.360459   \n",
      "1                                  0.739436                       0.581735   \n",
      "2                                  0.089336                       0.825776   \n",
      "3                                  0.453211                       0.896377   \n",
      "4                                  0.189920                       0.770917   \n",
      "\n",
      "   access_to_improved_water_sources  obesity_prevalence  \\\n",
      "0                          0.381366            0.339888   \n",
      "1                          0.566625            0.113026   \n",
      "2                          0.938416            0.595430   \n",
      "3                          0.910661            0.681353   \n",
      "4                          0.898450            0.621013   \n",
      "\n",
      "   access_to_electricity  co2_emissions  ratio_urban_population_total  \n",
      "0               0.514125       0.501945                      0.269132  \n",
      "1               0.762074       0.281082                      0.351608  \n",
      "2               0.893837       0.120386                      0.319509  \n",
      "3               0.971112       0.558871                      0.650169  \n",
      "4               0.901481       0.028361                      0.669631  \n",
      "row_id                                         int64\n",
      "country_code                                category\n",
      "year                                        category\n",
      "agricultural_land_area                       float64\n",
      "forest_area                                  float64\n",
      "total_land_area                              float64\n",
      "population_growth                            float64\n",
      "avg_value_of_food_production                 float64\n",
      "food_imports_as_share_of_merch_exports       float64\n",
      "gross_domestic_product_per_capita_ppp        float64\n",
      "per_capita_food_supply_variability           float64\n",
      "avg_supply_of_protein_of_animal_origin       float64\n",
      "caloric_energy_from_cereals_roots_tubers     float64\n",
      "access_to_improved_sanitation                float64\n",
      "access_to_improved_water_sources             float64\n",
      "obesity_prevalence                           float64\n",
      "access_to_electricity                        float64\n",
      "co2_emissions                                float64\n",
      "ratio_urban_population_total                 float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "final_scaled_normalised_training_values_filename = 'data/DAT102x_Predicting_Chronic_Hunger_-_Clean_Normal_Training_values.csv'\n",
    "training_labels_filename = 'data/DAT102x_Predicting_Chronic_Hunger_-_Training_labels.csv'\n",
    "final_scaled_normalised_test_values_filename = 'data/DAT102x_Predicting_Chronic_Hunger_-_Clean_Normal_Test_values.csv'\n",
    "\n",
    "training_values = pd.read_csv(final_scaled_normalised_training_values_filename)\n",
    "training_labels = pd.read_csv(training_labels_filename)\n",
    "test_values = pd.read_csv(final_scaled_normalised_test_values_filename)\n",
    "\n",
    "# Makes sure country_code and year are treated as categorical!\n",
    "training_values['country_code'] = training_values['country_code'].astype('category')\n",
    "training_values['year'] = training_values['year'].astype('category')\n",
    "test_values['country_code'] = test_values['country_code'].astype('category')\n",
    "test_values['year'] = test_values['year'].astype('category')\n",
    "\n",
    "print(\"Training values: {}\".format(training_values.shape))\n",
    "print(\"Training label: {}\".format(training_labels.shape))\n",
    "print(\"Test values:     {}\".format(test_values.shape))\n",
    "print(training_values.head())\n",
    "#print(training_values.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join training features and label into test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1311, 20)\n"
     ]
    }
   ],
   "source": [
    "tempDF = pd.merge(training_values, training_labels, on='row_id', how='inner')\n",
    "print(tempDF.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the test feature matrix and test label vector.\n",
    "Keep in mind that at this stage features are numerical and get_dummies doesn't actually do anything. I used it during development and saw no reason to remove it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1311, 32)\n",
      "agricultural_land_area                      float64\n",
      "forest_area                                 float64\n",
      "total_land_area                             float64\n",
      "population_growth                           float64\n",
      "avg_value_of_food_production                float64\n",
      "food_imports_as_share_of_merch_exports      float64\n",
      "gross_domestic_product_per_capita_ppp       float64\n",
      "per_capita_food_supply_variability          float64\n",
      "avg_supply_of_protein_of_animal_origin      float64\n",
      "caloric_energy_from_cereals_roots_tubers    float64\n",
      "access_to_improved_sanitation               float64\n",
      "access_to_improved_water_sources            float64\n",
      "obesity_prevalence                          float64\n",
      "access_to_electricity                       float64\n",
      "co2_emissions                               float64\n",
      "ratio_urban_population_total                float64\n",
      "year_2000                                     uint8\n",
      "year_2001                                     uint8\n",
      "year_2002                                     uint8\n",
      "year_2003                                     uint8\n",
      "year_2004                                     uint8\n",
      "year_2005                                     uint8\n",
      "year_2006                                     uint8\n",
      "year_2007                                     uint8\n",
      "year_2008                                     uint8\n",
      "year_2009                                     uint8\n",
      "year_2010                                     uint8\n",
      "year_2011                                     uint8\n",
      "year_2012                                     uint8\n",
      "year_2013                                     uint8\n",
      "year_2014                                     uint8\n",
      "year_2015                                     uint8\n",
      "dtype: object\n",
      "[31.26071279 18.29823274 39.51339713 ... 12.08848436 26.43666106\n",
      " 13.71256945]\n"
     ]
    }
   ],
   "source": [
    "# Start at 2nd column, i.e. exclude country_code\n",
    "X = pd.get_dummies(training_values.iloc[:,2:len(training_values)])\n",
    "y = tempDF['prevalence_of_undernourishment'].values\n",
    "print(X.shape)\n",
    "#print(X.dtypes)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use train/test split with different random_state values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(983, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create regressor using LBFGS (small training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPRegressor(activation='identity', alpha=0.0001, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=300, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=None, shuffle=True, solver='lbfgs', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "nn = MLPRegressor(activation='identity',\n",
    "                  hidden_layer_sizes=300,\n",
    "                  max_iter=500,\n",
    "                  verbose=False,\n",
    "                  solver='lbfgs')\n",
    "print(nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model and then use it to predict PoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.787673633282141\n"
     ]
    }
   ],
   "source": [
    "nn.fit(X_train, y_train)\n",
    "y_pred = nn.predict(X_test)\n",
    "print((metrics.mean_squared_error(y_test,y_pred))**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model sensitivy by running a 10 fold cross validation to get a sense of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.76 (+/- 2.43)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(nn, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "print(\"RMSE: %0.2f (+/- %0.2f)\" % ((abs(scores.mean())**0.5), (abs(scores.std())**0.5) * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPRegressor(activation='identity', alpha=0.0001, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=300, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=None, shuffle=True, solver='lbfgs', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to use a grid search to find \"best\" parameters\n",
    "# define the parameter values that should be searched\n",
    "hidden_layer_sizes_range = [300, 400, 500] # had 100, 200, 400, 500\n",
    "solver_options = [ 'lbfgs'] #, 'lbfgs', 'sgd',]\n",
    "activation_options = ['identity']#, 'logistic', 'tanh', 'relu']\n",
    "alpha_range = [0.0001]\n",
    "max_iter_range = [400, 500] # 200, 300, 500..\n",
    "#beta_1_range = [0.3, 0.5, 0.7, 0.9, 0.99]\n",
    "#beta_2_range = [0.5, 0.999]\n",
    "\n",
    "# create a parameter grid: map the parameter names to the values that should be searched\n",
    "param_grid = dict(hidden_layer_sizes=hidden_layer_sizes_range,\n",
    "                  solver=solver_options,\n",
    "                  alpha=alpha_range,\n",
    "                  activation=activation_options,\n",
    "                  max_iter=max_iter_range)\n",
    "#                  beta_1=beta_1_range,\n",
    "#                  beta_2=beta_2_range)\n",
    "#                  learning_rate_init=learning_rate_init_range,\n",
    "\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# instantiate and fit the grid\n",
    "grid = GridSearchCV(nn, param_grid, cv=10, \n",
    "                    scoring='neg_mean_squared_error', \n",
    "                    return_train_score=False,\n",
    "                    n_jobs = -1)\n",
    "grid.fit(X, y)\n",
    "\n",
    "#grid = GridSearchCV(nn, param_grid, cv=10, \n",
    "#                    scoring='neg_mean_squared_error', \n",
    "#                    return_train_score=False,\n",
    "#                    n_jobs = -1)\n",
    "\n",
    "# Best using lbfgs:\n",
    "#{'activation': 'identity', 'alpha': 0.0001, 'hidden_layer_sizes': 300, 'max_iter': 400, 'solver': 'lbfgs'}\n",
    "# Best using adam:\n",
    "#{'activation': 'identity', 'alpha': 0.0001, 'beta_1': 0.9, 'beta_2': 0.999, 'hidden_layer_sizes': 400, 'max_iter': 400, 'solver': 'adam'}\n",
    "\n",
    "# Choose to use lbfgs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(pd.DataFrame(grid.cv_results_)[['mean_test_score', 'std_test_score', 'params']])\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "access_to_improved_water_sources\n",
    "access_to_electricity\n",
    "obesity_prevalence\n",
    "access_to_improved_sanitation\n",
    "avg_supply_of_protein_of_animal_origin\n",
    "life_expectancy\n",
    "adult_literacy_rate\n",
    "avg_value_of_food_production\n",
    "\n",
    "droughts_floods_extreme_temps\n",
    "population_growth\n",
    "anemia_prevalence\n",
    "caloric_energy_from_cereals_roots_tubers\n",
    "net_oda_received_percent_gni\n",
    "open_defecation\n",
    "fertility_rate\n",
    "country_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All the testing below implies this is the BEST model, so use it to predict and check result?\n",
    "nn = MLPRegressor(activation='identity',\n",
    "                  hidden_layer_sizes=300,\n",
    "                  max_iter=500,\n",
    "                  verbose=False,\n",
    "                  solver='lbfgs')\n",
    "print(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5256444240817757\n"
     ]
    }
   ],
   "source": [
    "# Use the whole dataset to train the model\n",
    "nn.fit(X, y)\n",
    "y_pred = nn.predict(X)\n",
    "print((metrics.mean_squared_error(y,y_pred))**0.5)\n",
    "#print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_id                                        int64\n",
      "country_code                                 object\n",
      "year                                          int64\n",
      "agricultural_land_area                      float64\n",
      "forest_area                                 float64\n",
      "total_land_area                             float64\n",
      "population_growth                           float64\n",
      "avg_value_of_food_production                float64\n",
      "food_imports_as_share_of_merch_exports      float64\n",
      "gross_domestic_product_per_capita_ppp       float64\n",
      "per_capita_food_supply_variability          float64\n",
      "avg_supply_of_protein_of_animal_origin      float64\n",
      "caloric_energy_from_cereals_roots_tubers    float64\n",
      "access_to_improved_sanitation               float64\n",
      "access_to_improved_water_sources            float64\n",
      "obesity_prevalence                          float64\n",
      "access_to_electricity                       float64\n",
      "co2_emissions                               float64\n",
      "ratio_urban_population_total                float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#print(test_values.shape)\n",
    "#print(test_values.head())\n",
    "print(test_values.dtypes)\n",
    "#X_test = pd.get_dummies(test_values).iloc[:,2:-1].values\n",
    "#print(X)\n",
    "#print(X_test)\n",
    "#X_pred = pd.get_dummies(test_values).iloc[:,4:-1] #.values\n",
    "#X_pred = test_values.iloc[:,2:-1] #.values\n",
    "#print(X_pred.shape)\n",
    "#print(X_pred)\n",
    "#y_pred = nn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(pd.Series(y_pred), columns=['prevalence_of_undernourishment'])\n",
    "row = pd.DataFrame(test_values['row_id'].copy(), columns=['row_id'])\n",
    "res = pd.concat([row, pred], axis='columns')\n",
    "print(type(pred))\n",
    "print(type(row))\n",
    "print(type(res))\n",
    "print(res.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('data/prediction-20181030-03.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
